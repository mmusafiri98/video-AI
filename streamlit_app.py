import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

st.title("ü§ñ Text-Generation CPU Demo")

model_name = "distilgpt2"  # Mod√®le l√©ger
device = "cpu"

@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    model.to(device)
    return tokenizer, model

tokenizer, model = load_model()

prompt = st.text_area("√âcris ton texte ici :", "Bonjour, comment √ßa va ?")

max_length = st.slider("Longueur max de g√©n√©ration", 20, 200, 50)

if st.button("G√©n√©rer"):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with st.spinner("G√©n√©ration en cours..."):
        output = model.generate(inputs["input_ids"], max_length=max_length)
    text = tokenizer.decode(output[0], skip_special_tokens=True)
    st.text_area("Texte g√©n√©r√© :", text, height=200)

