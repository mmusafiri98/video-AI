import streamlit as st
from PIL import Image
import torch
from diffusers import DiffusionPipeline
import imageio

# Charger un modÃ¨le image â†’ image (plus lÃ©ger que vidÃ©o complÃ¨te)
@st.cache_resource
def load_pipeline():
    pipe = DiffusionPipeline.from_pretrained(
        "runwayml/stable-diffusion-v1-5",
        torch_dtype=torch.float32
    )
    pipe = pipe.to("cpu")  # CPU uniquement
    return pipe

st.title("ğŸ¬ GÃ©nÃ©rateur VidÃ©o (CPU-friendly)")

uploaded_file = st.file_uploader("ğŸ“¤ Uploade une image de dÃ©part", type=["png", "jpg", "jpeg"])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="Image de dÃ©part", use_container_width=True)

    prompt = st.text_input("ğŸ“ DÃ©cris la transformation Ã  appliquer", "Un paysage futuriste cyberpunk")

    if st.button("ğŸš€ GÃ©nÃ©rer la vidÃ©o (courte animation)"):
        pipe = load_pipeline()

        frames = []
        with st.spinner("GÃ©nÃ©ration en cours... â³ (Ã§a peut prendre ~1-2 minutes)"):
            for i in range(8):  # 8 frames max pour rester lÃ©ger
                frame = pipe(prompt=prompt, image=image, strength=0.6, guidance_scale=7.5).images[0]
                frames.append(frame)

        output_path = "output.mp4"
        imageio.mimsave(output_path, frames, fps=4)  # fps bas pour compresser

        st.video(output_path)
        st.success("âœ… VidÃ©o gÃ©nÃ©rÃ©e avec succÃ¨s !")
